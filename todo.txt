
1. add deepmind wrapper for osu env (frameskip/framestacking, rgb->gs?)
2. add standard functions for env i.e get_random_action, etc
3. (done!) make it so rewards in env.step() return incremental rewards, not the entire score at each time step
4. resize images to lower res if faster computation time is needed, does not affect vision-scoring algorithm
5. (done!) modify env so it supports automated multi epoch training
6. try ray actor critic agents, implement my own after they are confirmed to work

issues to fix:
    * (fixed!) add a more consistent way of determining the 'done' parameter in env.step()
    * (fixed!) first few digits in the scoring function have consistent mistakes 
    * pyautogui literally does not move around in osu (need a different package)
    * (fixed!) new vision algo mistakes 1 for 4 (hardcode blue and white individually?) 
        * optimized vision scoring algo, significantly reduced computation time by optimized data structures used
        * updated assets for osu and mcosu static files
    * random agent clicks so rapidly sometimes that it will accidentally immediately click the restart button without the environment getting a chance to reset
    
observations:
    * calculating rewards on a per-frame basis results in certain time steps giving very little - no rewards,
      not sure if this will cause issue with agent training though (i.e lag between a hit and the score updating may
      result in 0 reward at a given timestep